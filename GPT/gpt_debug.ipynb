{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一. config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out-test'\n",
    "eval_interval = 500\n",
    "log_interval = 4\n",
    "eval_iters = 10\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'shakespeare-char'\n",
    "wandb_run_name = 'mini-gpt' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'shakespeare_char'\n",
    "gradient_accumulation_steps = 4 # used to simulate larger batch sizes\n",
    "batch_size = 16 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 256  #text\n",
    "# 我很开心给\n",
    "# 大家做饭<eos>\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.1 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 5000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "lr_decay_iters = 5000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' \n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' \n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_offset = 0\n",
    "# os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "# device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "device_type = 'cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/shakespeare_char'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join('data', dataset)\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=1024,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "if meta_vocab_size is None:\n",
    "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "gptconf = GPTConfig(**model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二.load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 85.00M\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CasualSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=65, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GPT(gptconf).to(\"cpu\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([16, 256])\n",
      "Y: torch.Size([16, 256])\n",
      "X[0,:10]:  tensor([ 1, 40, 43,  0, 42, 39, 51, 52, 43, 42])\n",
      "Y[0,:10]:  tensor([40, 43,  0, 42, 39, 51, 52, 43, 42,  1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.load('X.tensor').to('cpu')\n",
    "Y = torch.load('Y.tensor').to('cpu')\n",
    "print('X:',X.shape)\n",
    "print('Y:',Y.shape)\n",
    "print('X[0,:10]: ', X[0,:10])\n",
    "print('Y[0,:10]: ', Y[0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPTConfig:\n",
    "batch_size = 16 \n",
    "block_size = 1024  # lenght\n",
    "n_layer = 2\n",
    "n_head = 4\n",
    "n_embd = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四 GPT forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(1024, 768)\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.wpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:256 = block_size:1024\n",
      "-----embding-input-------\n",
      "词嵌入向量维n_embd =  128\n",
      "tok_emb: torch.Size([16, 256, 768])\n",
      "pos_emb: torch.Size([256, 768])\n",
      "tok_emb+pos_emb: torch.Size([16, 256, 768])\n",
      "编码后embding input: torch.Size([16, 256, 768])\n",
      "-----decoder-block-------\n",
      "n_layer: 2\n",
      "decoder layers: 12\n",
      "0\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "1\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "2\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "3\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "4\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "5\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "6\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "7\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "8\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "9\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "10\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "11\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "ln_f x: torch.Size([16, 256, 768])\n",
      "-----lm_head-------\n",
      "lm_head : torch.Size([16, 256, 65])\n",
      "lm_head输出与解码词汇量相同, meta_vocab_size= 65\n",
      "-----loss-------\n",
      "tensor(4.4475, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "b,t = X.size()\n",
    "print(\"t:{} = block_size:{}\".format(t, block_size))\n",
    "pos = torch.arange(0, t, dtype=torch.long)\n",
    "# forward the GPT model itself\n",
    "tok_emb = model.transformer.wte(X) # token embeddings of shape (b, t, n_embd)\n",
    "pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "print('-----embding-input-------')\n",
    "print('词嵌入向量维n_embd = ', n_embd)\n",
    "print('tok_emb:', tok_emb.shape)\n",
    "print('pos_emb:', pos_emb.shape)\n",
    "print('tok_emb+pos_emb:', (tok_emb + pos_emb).shape)\n",
    "x = model.transformer.drop(tok_emb + pos_emb) # pretraining \n",
    "x_enc = model.transformer.drop(tok_emb + pos_emb)\n",
    "print('编码后embding input:', x.shape)\n",
    "print('-----decoder-block-------')\n",
    "print('n_layer:', n_layer)\n",
    "print('decoder layers:', len(model.transformer.h))\n",
    "i=0\n",
    "for block in model.transformer.h:\n",
    "    print(i)\n",
    "    i+=1\n",
    "    x = block(x)\n",
    "    print('decoder x:', x.shape)\n",
    "x = model.transformer.ln_f(x)\n",
    "print('ln_f x:', x.shape)\n",
    "print('-----lm_head-------')\n",
    "logits = model.lm_head(x)\n",
    "print('lm_head :', logits.shape)\n",
    "print('lm_head输出与解码词汇量相同, meta_vocab_size=',meta_vocab_size )\n",
    "print('-----loss-------')\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------decoder block-------------\n",
      "Block(\n",
      "  (ln_1): LayerNorm()\n",
      "  (attn): CasualSelfAttention(\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm()\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
      "    (gelu): GELU(approximate='none')\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm : torch.Size([16, 256, 768])\n",
      "masked_self_attention : torch.Size([16, 256, 768])\n",
      "layer norm : torch.Size([16, 256, 768])\n",
      "mlp : torch.Size([16, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "print('-------------decoder block-------------')\n",
    "decoder_block = model.transformer.h[0]\n",
    "print(decoder_block)\n",
    "\n",
    "x = x_enc\n",
    "x_ln_1 = decoder_block.ln_1(x)\n",
    "x_attn = decoder_block.attn(x_ln_1)\n",
    "x_ln_2 = decoder_block.ln_2(x_attn)\n",
    "x_mlp = decoder_block.mlp(x_ln_2)\n",
    "\n",
    "print('layer norm :', x_ln_1.shape)\n",
    "print('masked_self_attention :', x_attn.shape)\n",
    "print('layer norm :', x_ln_2.shape)\n",
    "print('mlp :', x_mlp.shape)\n",
    "\n",
    "x = x + decoder_block.attn(decoder_block.ln_1(x))\n",
    "x = x + decoder_block.mlp(decoder_block.ln_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CasualSelfAttention(\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "如果torch>2.0.0, 是否可直接使用scaled_dot_product_attention： True\n",
      "batch:16, block:256, embed:768, \n",
      "---------------1. 将嵌入向量传播成3*n_embd--------------\n",
      "n_embed: 768\n",
      "n_embed*3: 2304\n",
      "x_liner: torch.Size([16, 256, 2304])\n",
      "---------------2. 将3*n_embd split成QKV--------------\n",
      "split: q: torch.Size([16, 256, 768])\n",
      "---------------3. 将QKV拆分 多头QKV--------------\n",
      "n_embed:768 / n_head:12 = 64 \n",
      "q: torch.Size([16, 12, 256, 64])\n",
      "Q = batch:16, n_head:4, block:256, head_embed:64 \n",
      "---------------4.多头计算attention，直接使用torch function--------------\n",
      "y: torch.Size([16, 12, 256, 64])\n",
      "---------------5.将多头注意力结果拼接--------------\n",
      "y-concat: torch.Size([16, 256, 768])\n",
      "---------------6. 增加一次前向传播--------------\n",
      "y_proj: torch.Size([16, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# Masked Self Attention\n",
    "attention = model.transformer.h[0].attn\n",
    "print(attention)\n",
    "print(\"如果torch>2.0.0, 是否可直接使用scaled_dot_product_attention：\",attention.flash)\n",
    "\n",
    "x = x_ln_1\n",
    "B, T, C = x.size()\n",
    "print(\"batch:{}, block:{}, embed:{}, \".format(B, T, C))\n",
    "\n",
    "# self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "print('---------------1. 将嵌入向量传播成3*n_embd--------------')\n",
    "x_liner = attention.c_attn(x)\n",
    "print(\"n_embed:\",attention.n_embd)\n",
    "print(\"n_embed*3:\",attention.n_embd*3)\n",
    "print(\"x_liner:\", x_liner.shape)\n",
    "\n",
    "print('---------------2. 将3*n_embd split成QKV--------------')\n",
    "q, k, v  = x_liner.split(attention.n_embd, dim=2)\n",
    "print(\"split: q:\", q.shape)\n",
    "\n",
    "print('---------------3. 将QKV拆分 多头QKV--------------')\n",
    "print(\"n_embed:{} / n_head:{} = {} \".format(C, attention.n_head, C//attention.n_head))\n",
    "k = k.view(B, T, attention.n_head, C // attention.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "q = q.view(B, T, attention.n_head, C // attention.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "v = v.view(B, T, attention.n_head, C // attention.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "print(\"q:\",q.shape)\n",
    "print(\"Q = batch:{}, n_head:{}, block:{}, head_embed:{} \".format(B, n_head, T, C//attention.n_head))\n",
    "\n",
    "\n",
    "print('---------------4.多头计算attention，直接使用torch function--------------')\n",
    "y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, \n",
    "                                                     dropout_p=attention.dropout \n",
    "                                                     if attention.training \n",
    "                                                     else 0, is_causal=True)\n",
    "print('y:', y.shape)\n",
    "\n",
    "\n",
    "print('---------------5.将多头注意力结果拼接--------------')\n",
    "y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "print('y-concat:', y.shape)\n",
    "\n",
    "\n",
    "print('---------------6. 增加一次前向传播--------------')\n",
    "y = attention.resid_dropout(attention.c_proj(y))\n",
    "print(\"y_proj:\", y.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "masked_matrix = torch.tril(torch.ones(T,T)).view(1, 1, T, T)\n",
    "print(masked_matrix[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5行代码实现多头注意力计算\n",
      "q score: torch.Size([16, 12, 256, 64])\n",
      "k score: torch.Size([16, 12, 256, 64])\n",
      "k_t score: torch.Size([16, 12, 64, 256])\n",
      "q @ k_t score: torch.Size([16, 12, 256, 256])\n",
      "attn score: torch.Size([16, 12, 256, 256])\n",
      "attn score: torch.Size([16, 12, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "# 不使用torch2.0 attention计算\n",
    "print('5行代码实现多头注意力计算')\n",
    "# 1. scale and dot product process\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "print(\"q score:\", q.shape)\n",
    "print(\"k score:\", k.shape)\n",
    "print(\"k_t score:\", k.transpose(-2, -1).shape)\n",
    "print(\"q @ k_t score:\", (q@k.transpose(-2, -1)).shape)\n",
    "print(\"attn score:\", att.shape)\n",
    "# 2. Mask为下三角矩阵\n",
    "att = att.masked_fill(masked_matrix == 0, float('-inf'))\n",
    "# 3. softmax\n",
    "att = F.softmax(att, dim=-1)\n",
    "# 4. attn\n",
    "att = attention.attn_dropout(att)\n",
    "# 5. score\n",
    "y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "print(\"attn score:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "x: torch.Size([16, 256, 768])\n",
      "x_fc: torch.Size([16, 256, 3072])\n",
      "x_proj: torch.Size([16, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# mlp实现\n",
    "mlp = model.transformer.h[0].mlp\n",
    "print(mlp)\n",
    "print(\"x:\",x.shape)\n",
    "x = mlp.c_fc(x)\n",
    "print(\"x_fc:\",x.shape)\n",
    "x = mlp.gelu(x)\n",
    "x = mlp.c_proj(x)\n",
    "print(\"x_proj:\",x.shape)\n",
    "x = mlp.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 768])\n",
      "torch.Size([16, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# layer normalization\n",
    "ln = model.transformer.h[0].ln_1\n",
    "print(x.shape)\n",
    "F.layer_norm(x, ln.weight.shape, ln.weight, ln.bias, 1e-5)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五. generate by gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测词表大小 65\n",
      "输入X: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1]])\n",
      "输入X长度: torch.Size([1, 10])\n",
      "输出Logits: torch.Size([1, 1, 65])\n",
      "输出Probs: torch.Size([1, 1, 65])\n",
      "tensor([[[0.0053, 0.0268, 0.0201, 0.0132, 0.0070, 0.0154, 0.0224, 0.0134,\n",
      "          0.0132, 0.0222, 0.0237, 0.0077, 0.0060, 0.0079, 0.0111, 0.0130,\n",
      "          0.0067, 0.0166, 0.0205, 0.0240, 0.0371, 0.0167, 0.0377, 0.0282,\n",
      "          0.0083, 0.0087, 0.0166, 0.0149, 0.0058, 0.0213, 0.0206, 0.0125,\n",
      "          0.0538, 0.0059, 0.0346, 0.0130, 0.0101, 0.0138, 0.0098, 0.0278,\n",
      "          0.0125, 0.0158, 0.0063, 0.0229, 0.0186, 0.0097, 0.0064, 0.0060,\n",
      "          0.0047, 0.0179, 0.0181, 0.0176, 0.0072, 0.0140, 0.0082, 0.0052,\n",
      "          0.0084, 0.0115, 0.0072, 0.0160, 0.0185, 0.0136, 0.0186, 0.0144,\n",
      "          0.0070]]], grad_fn=<SoftmaxBackward0>)\n",
      "输出下一个Token: torch.Size([1, 1])\n",
      "tensor([[32]])\n",
      "当前的token长度: 11\n",
      "当前的token序列: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 32]])\n",
      "输出Logits: torch.Size([1, 1, 65])\n",
      "输出Probs: torch.Size([1, 1, 65])\n",
      "tensor([[[0.0034, 0.0060, 0.0127, 0.0132, 0.0072, 0.0183, 0.0122, 0.0119,\n",
      "          0.0149, 0.0400, 0.0204, 0.0165, 0.0094, 0.0084, 0.0152, 0.0106,\n",
      "          0.0069, 0.0228, 0.0184, 0.0229, 0.0168, 0.0126, 0.0196, 0.0269,\n",
      "          0.0062, 0.0077, 0.0139, 0.0240, 0.0100, 0.0155, 0.0275, 0.0109,\n",
      "          0.0457, 0.0043, 0.0158, 0.0085, 0.0138, 0.0157, 0.0154, 0.0271,\n",
      "          0.0173, 0.0393, 0.0087, 0.0285, 0.0187, 0.0193, 0.0056, 0.0104,\n",
      "          0.0095, 0.0232, 0.0213, 0.0141, 0.0044, 0.0117, 0.0149, 0.0037,\n",
      "          0.0056, 0.0196, 0.0088, 0.0253, 0.0170, 0.0123, 0.0135, 0.0146,\n",
      "          0.0035]]], grad_fn=<SoftmaxBackward0>)\n",
      "输出下一个Token: torch.Size([1, 1])\n",
      "tensor([[32]])\n",
      "当前的token长度: 12\n",
      "当前的token序列: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 32, 32]])\n"
     ]
    }
   ],
   "source": [
    "# Greedy Generation\n",
    "idx = X[1,:10].reshape(1, 10) # generate 1\n",
    "print(\"预测词表大小\", model.config.vocab_size) # BPE词表\n",
    "print(\"输入X:\", idx)\n",
    "print(\"输入X长度:\", idx.shape)\n",
    "for _ in range(2):\n",
    "    logits, _ = model(idx)\n",
    "    print('输出Logits:', logits.shape)     \n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print('输出Probs:', probs.shape)\n",
    "    print(probs)\n",
    "    idx_next = torch.argmax(probs , dim=2) \n",
    "    print('输出下一个Token:', idx_next.shape)\n",
    "    print(idx_next)\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "    # idx = idx_next\n",
    "    print(\"当前的token长度:\", len(idx[0]))\n",
    "    print(\"当前的token序列:\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "torch.set_printoptions(linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "65\n",
      "prompt: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1]])\n",
      "idx_cond: torch.Size([1, 10])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[ 1,  6, 50, 44, 34]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf, 0.9813,   -inf,   -inf,   -inf,   -inf, 0.9209,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf, 0.6405,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "         0.7118,   -inf,   -inf,   -inf,   -inf,   -inf, 0.7409,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.2379, 0.0000, 0.0000, 0.0000, 0.0000, 0.2240, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1692, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1817, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1871, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[1]])\n",
      "torch.Size([1, 11])\n",
      "generate: length : 11\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1]])\n",
      "idx_cond: torch.Size([1, 11])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[40, 41, 57, 39, 34]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf, 0.8338,   -inf,   -inf,   -inf,   -inf, 0.8995, 1.1118, 1.0604,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf, 1.0145,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1712, 0.0000, 0.0000, 0.0000, 0.0000, 0.1828, 0.2261, 0.2148, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[34]])\n",
      "torch.Size([1, 12])\n",
      "generate: length : 12\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34]])\n",
      "idx_cond: torch.Size([1, 12])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[34, 19,  9, 57, 17]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 1.0527,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.9836,   -inf, 1.1513,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf, 1.1650,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf, 0.9903,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1963, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1832, 0.0000, 0.2166, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1844, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[17]])\n",
      "torch.Size([1, 13])\n",
      "generate: length : 13\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34, 17]])\n",
      "idx_cond: torch.Size([1, 13])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[17, 41, 10, 30, 23]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.9727,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 1.7198,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf, 0.7218,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.8596,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 1.4285,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1572,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3319, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1404, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2480, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[10]])\n",
      "torch.Size([1, 14])\n",
      "generate: length : 14\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34, 17, 10]])\n",
      "idx_cond: torch.Size([1, 14])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[34,  9, 43, 10, 23]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.9286, 0.8664,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf, 0.7804,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf, 1.2021,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.9255,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1956, 0.1838,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2571, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1950,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[43]])\n",
      "torch.Size([1, 15])\n",
      "generate: length : 15\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34, 17, 10, 43]])\n",
      "idx_cond: torch.Size([1, 15])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[43, 59,  3, 20, 56]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf, 0.8273,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.6285,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 1.1275,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf, 0.6058,   -inf,   -inf, 1.0528,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1914, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2584,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1534, 0.0000, 0.0000, 0.2398, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[59]])\n",
      "torch.Size([1, 16])\n",
      "generate: length : 16\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34, 17, 10, 43, 59]])\n",
      "idx_cond: torch.Size([1, 16])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[59, 17, 41, 63,  9]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.8269,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 1.2502,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.9470,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf, 1.7919,   -inf,   -inf,   -inf, 0.8794,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1364, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2082, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1538, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.3579, 0.0000, 0.0000, 0.0000, 0.1437, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[41]])\n",
      "torch.Size([1, 17])\n",
      "generate: length : 17\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34, 17, 10, 43, 59, 41]])\n",
      "idx_cond: torch.Size([1, 17])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[41,  9, 37, 59, 14]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.9293,   -inf,\n",
      "           -inf,   -inf,   -inf, 0.6073,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf, 0.7731,   -inf,   -inf,   -inf, 1.9857,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf, 0.6249,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1615, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1170, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1381, 0.0000, 0.0000, 0.0000, 0.4643, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1191, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[41]])\n",
      "torch.Size([1, 18])\n",
      "generate: length : 18\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34, 17, 10, 43, 59, 41, 41]])\n",
      "idx_cond: torch.Size([1, 18])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[41,  9, 31, 14, 37]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 1.1666,   -inf,\n",
      "           -inf,   -inf,   -inf, 0.9709,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.9867,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf, 0.9553,   -inf,   -inf,   -inf, 2.2561,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1553, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1277, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1297, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1257, 0.0000, 0.0000, 0.0000, 0.4616, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[41]])\n",
      "torch.Size([1, 19])\n",
      "generate: length : 19\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34, 17, 10, 43, 59, 41, 41, 41]])\n",
      "idx_cond: torch.Size([1, 19])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "i tensor([[41, 59, 17, 43, 29]])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 1.2031,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.7575,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 2.3406,   -inf, 0.8776,\n",
      "           -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
      "           -inf,   -inf,   -inf,   -inf, 1.3768,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1499, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0960, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4675, 0.0000, 0.1082,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1783, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[41]])\n",
      "torch.Size([1, 20])\n",
      "generate: length : 20\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1,  1, 34, 17, 10, 43, 59, 41, 41, 41, 41]])\n"
     ]
    }
   ],
   "source": [
    "# model.generate()\n",
    "# top-k GPT-2\n",
    "idx = X[1,:10].reshape(1, 10) # generate 1\n",
    "print(idx.shape)\n",
    "temperature = 1.0\n",
    "top_k = 5\n",
    "print(model.config.vocab_size) # BPE词表\n",
    "print(\"prompt:\", idx)\n",
    "\n",
    "#top k的方法\n",
    "# 词表 65\n",
    "# llama 32000\n",
    "\n",
    "for _ in range(10):\n",
    "    idx_cond = idx if idx.size(1) <= model.config.block_size else idx[:, -model.config.block_size:]\n",
    "    print('idx_cond:', idx_cond.shape)\n",
    "    \n",
    "    logits, _ = model(idx_cond)\n",
    "    print('logits:', logits.shape) \n",
    "    \n",
    "    print(\"no - 1\",logits[:, -1, :].shape)##为什么要-1来降维\n",
    "    \n",
    "    logits = logits[:, -1, :] / temperature #平缓， tips : 知识蒸馏[温度]\n",
    "    print('logits:', logits.shape)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        v, i = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        print('top_k v:', v.shape)\n",
    "        print('i', i)\n",
    "        \n",
    "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        print('top_k logits:', logits.shape)\n",
    "        print(logits)\n",
    "        \n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print('probs :', probs.shape)\n",
    "    print(probs)\n",
    "    \n",
    "    idx_next = torch.multinomial(probs, num_samples=1) #  num_samples-by-num_samples\n",
    "    print('idx_next :', idx_next.shape)\n",
    "    print(idx_next)\n",
    "    \n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "    print(idx.shape)\n",
    "    \n",
    "    print(\"generate: length :\", len(idx[0]))\n",
    "    print(\"generate:\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob :  tensor([0.3000, 0.3000, 0.0000])\n",
      "根据概率权重所选择next token的下标\n",
      "第0次sample的next_token下标为0: l\n",
      "第1次sample的next_token下标为0: l\n",
      "第2次sample的next_token下标为1: j\n",
      "第3次sample的next_token下标为0: l\n",
      "第4次sample的next_token下标为0: l\n",
      "第5次sample的next_token下标为1: j\n",
      "第6次sample的next_token下标为1: j\n",
      "第7次sample的next_token下标为0: l\n",
      "第8次sample的next_token下标为1: j\n",
      "第9次sample的next_token下标为1: j\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "dict_map = {0:\"l\", 1:\"j\", 2:\"h\"}\n",
    "prob = torch.tensor([0.3, 0.3, 0.0])\n",
    "print(\"prob : \",prob)\n",
    "\n",
    "print(\"根据概率权重所选择next token的下标\")\n",
    "for i in range(10):\n",
    "    next_token = torch.multinomial(prob, num_samples=1)[0]\n",
    "    print(f\"第{i}次sample的next_token下标为{next_token}:\", \n",
    "          dict_map[int(next_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob :  tensor([0.7000, 0.2000, 0.1000])\n",
      "prob/T :  tensor([0.3500, 0.1000, 0.0500])\n",
      "softmax(prob/T) :  tensor([0.3969, 0.3091, 0.2940])\n",
      "根据概率权重所选择next token的下标\n",
      "第0次sample的next_token下标为2: h\n",
      "第1次sample的next_token下标为1: j\n",
      "第2次sample的next_token下标为0: l\n",
      "第3次sample的next_token下标为2: h\n",
      "第4次sample的next_token下标为0: l\n",
      "第5次sample的next_token下标为2: h\n",
      "第6次sample的next_token下标为1: j\n",
      "第7次sample的next_token下标为1: j\n",
      "第8次sample的next_token下标为0: l\n",
      "第9次sample的next_token下标为0: l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57772/1159029707.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(prob)\n"
     ]
    }
   ],
   "source": [
    "# Do Sample with temparature\n",
    "temparature = 2.0\n",
    "dict_map = {0:\"l\", 1:\"j\", 2:\"h\"}\n",
    "prob = torch.tensor([0.7, 0.2, 0.1])\n",
    "print(\"prob : \",prob)\n",
    "prob /= temparature\n",
    "print(\"prob/T : \", prob)\n",
    "prob = F.softmax(prob)\n",
    "print(\"softmax(prob/T) : \", prob)\n",
    "\n",
    "print(\"根据概率权重所选择next token的下标\")\n",
    "for i in range(10):\n",
    "    next_token = torch.multinomial(prob, num_samples=1)[0]\n",
    "    print(f\"第{i}次sample的next_token下标为{next_token}:\", \n",
    "          dict_map[int(next_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob :  tensor([0.7000, 0.2000, 0.1000])\n",
      "prob/T :  tensor([0.3500, 0.1000, 0.0500])\n",
      "softmax(prob/T) :  tensor([0.3969, 0.3091, 0.2940])\n",
      "top-k: tensor([0.3969, 0.3091])\n",
      "top-k softmax: tensor([0.5219, 0.4781])\n",
      "根据概率权重所选择next token的下标\n",
      "第0次sample的next_token下标为1: j\n",
      "第1次sample的next_token下标为1: j\n",
      "第2次sample的next_token下标为1: j\n",
      "第3次sample的next_token下标为1: j\n",
      "第4次sample的next_token下标为0: l\n",
      "第5次sample的next_token下标为0: l\n",
      "第6次sample的next_token下标为0: l\n",
      "第7次sample的next_token下标为0: l\n",
      "第8次sample的next_token下标为1: j\n",
      "第9次sample的next_token下标为0: l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57772/3610843979.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(prob)\n",
      "/tmp/ipykernel_57772/3610843979.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(prob)\n"
     ]
    }
   ],
   "source": [
    "# top-k\n",
    "temparature = 2.0\n",
    "top_k = 2\n",
    "dict_map = {0:\"l\", 1:\"j\", 2:\"h\"}\n",
    "prob = torch.tensor([0.7, 0.2, 0.1])\n",
    "print(\"prob : \",prob)\n",
    "prob /= temparature\n",
    "print(\"prob/T : \", prob)\n",
    "prob = F.softmax(prob)\n",
    "print(\"softmax(prob/T) : \", prob)\n",
    "prob, _ = torch.topk(prob, top_k)\n",
    "print(\"top-k:\", prob)\n",
    "prob = F.softmax(prob)\n",
    "print(\"top-k softmax:\", prob)\n",
    "print(\"根据概率权重所选择next token的下标\")\n",
    "for i in range(10):\n",
    "    next_token = torch.multinomial(prob, num_samples=1)[0]\n",
    "    print(f\"第{i}次sample的next_token下标为{next_token}:\", \n",
    "          dict_map[int(next_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1]])\n",
      "tensor([[-0.7929,  0.5534, -0.1229,  0.3507, -1.3380,  0.4355,  0.4515, -0.1907, -0.8722,  1.0584,\n",
      "         -0.1178, -0.4903, -0.2119, -0.6337,  0.4863, -0.4952,  0.0203,  0.8240, -0.1047,  1.1403,\n",
      "          0.2497, -0.2922,  0.2068,  0.2550,  0.1722, -0.0060,  0.5095, -0.0022, -0.0195,  0.4862,\n",
      "          0.9912, -0.7361,  0.7733, -1.0519,  0.2914,  0.2043, -0.1635,  0.2698, -0.5187,  0.2467,\n",
      "         -0.2317,  0.5831, -0.4360,  0.6017, -0.1734, -0.1365, -0.3749,  0.3584, -1.1514,  0.1052,\n",
      "          0.3275, -0.7800, -0.5172,  0.2508, -0.5244, -0.3392, -0.0874,  0.2741, -0.7231,  0.1629,\n",
      "         -0.1338,  0.5089,  0.7228,  0.2831, -0.6781]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-1.5857,  0.2767, -0.1229,  0.3507, -1.3380,  0.4355,  0.4515, -0.1907, -0.8722,  1.0584,\n",
      "         -0.1178, -0.4903, -0.2119, -1.2675,  0.4863, -0.4952,  0.0203,  0.8240, -0.1047,  1.1403,\n",
      "          0.2497, -0.2922,  0.2068,  0.2550,  0.1722, -0.0060,  0.2547, -0.0022, -0.0195,  0.4862,\n",
      "          0.9912, -0.7361,  0.7733, -1.0519,  0.2914,  0.2043, -0.1635,  0.2698, -0.5187,  0.2467,\n",
      "         -0.2317,  0.5831, -0.8719,  0.3008, -0.1734, -0.1365, -0.3749,  0.3584, -1.1514,  0.1052,\n",
      "          0.3275, -0.7800, -1.0344,  0.1254, -0.5244, -0.3392, -0.1747,  0.2741, -0.7231,  0.1629,\n",
      "         -0.1338,  0.5089,  0.7228,  0.2831, -0.6781]], grad_fn=<AsStridedBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7929, 0.2767, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.6337, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.2547, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4360, 0.3008,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5172, 0.1254, 0.0000,\n",
       "         0.0000, 0.0874, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeatition penalty\n",
    "idx = X[1,:10].reshape(1, 10) \n",
    "print(idx)\n",
    "penalty = 2.0\n",
    "for _ in range(256):\n",
    "    logits,_ = model(idx)\n",
    "    logits = logits[:,-1,:]\n",
    "    origin_logits = logits.clone()\n",
    "\n",
    "    print(logits)\n",
    "    # repetition penalty\n",
    "    logits_idx = torch.gather(logits,1,idx)\n",
    "    logits_idx = torch.where(logits_idx < 0,logits_idx * penalty,logits_idx / penalty)\n",
    "    logits = logits.scatter_(1,idx,logits_idx)\n",
    "    print(logits)\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "    break\n",
    "origin_logits - logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob :  tensor([0.3000, 0.3000, 0.0000])\n",
      "根据概率权重所选择next token的下标\n",
      "第0次sample的next_token下标为0: l\n",
      "第1次sample的next_token下标为1: j\n",
      "第2次sample的next_token下标为0: l\n",
      "第3次sample的next_token下标为0: l\n",
      "第4次sample的next_token下标为1: j\n",
      "第5次sample的next_token下标为0: l\n",
      "第6次sample的next_token下标为0: l\n",
      "第7次sample的next_token下标为1: j\n",
      "第8次sample的next_token下标为0: l\n",
      "第9次sample的next_token下标为0: l\n",
      "第10次sample的next_token下标为0: l\n",
      "第11次sample的next_token下标为1: j\n",
      "第12次sample的next_token下标为1: j\n",
      "第13次sample的next_token下标为0: l\n",
      "第14次sample的next_token下标为1: j\n",
      "第15次sample的next_token下标为1: j\n",
      "第16次sample的next_token下标为0: l\n",
      "第17次sample的next_token下标为1: j\n",
      "第18次sample的next_token下标为1: j\n",
      "第19次sample的next_token下标为0: l\n",
      "第20次sample的next_token下标为1: j\n",
      "第21次sample的next_token下标为1: j\n",
      "第22次sample的next_token下标为0: l\n",
      "第23次sample的next_token下标为1: j\n",
      "第24次sample的next_token下标为1: j\n",
      "第25次sample的next_token下标为0: l\n",
      "第26次sample的next_token下标为0: l\n",
      "第27次sample的next_token下标为1: j\n",
      "第28次sample的next_token下标为1: j\n",
      "第29次sample的next_token下标为0: l\n",
      "第30次sample的next_token下标为0: l\n",
      "第31次sample的next_token下标为0: l\n",
      "第32次sample的next_token下标为1: j\n",
      "第33次sample的next_token下标为0: l\n",
      "第34次sample的next_token下标为1: j\n",
      "第35次sample的next_token下标为0: l\n",
      "第36次sample的next_token下标为1: j\n",
      "第37次sample的next_token下标为0: l\n",
      "第38次sample的next_token下标为0: l\n",
      "第39次sample的next_token下标为0: l\n",
      "第40次sample的next_token下标为1: j\n",
      "第41次sample的next_token下标为0: l\n",
      "第42次sample的next_token下标为1: j\n",
      "第43次sample的next_token下标为0: l\n",
      "第44次sample的next_token下标为1: j\n",
      "第45次sample的next_token下标为1: j\n",
      "第46次sample的next_token下标为1: j\n",
      "第47次sample的next_token下标为0: l\n",
      "第48次sample的next_token下标为1: j\n",
      "第49次sample的next_token下标为1: j\n",
      "第50次sample的next_token下标为1: j\n",
      "第51次sample的next_token下标为0: l\n",
      "第52次sample的next_token下标为0: l\n",
      "第53次sample的next_token下标为0: l\n",
      "第54次sample的next_token下标为0: l\n",
      "第55次sample的next_token下标为0: l\n",
      "第56次sample的next_token下标为1: j\n",
      "第57次sample的next_token下标为0: l\n",
      "第58次sample的next_token下标为0: l\n",
      "第59次sample的next_token下标为0: l\n",
      "第60次sample的next_token下标为0: l\n",
      "第61次sample的next_token下标为0: l\n",
      "第62次sample的next_token下标为0: l\n",
      "第63次sample的next_token下标为1: j\n",
      "第64次sample的next_token下标为1: j\n",
      "第65次sample的next_token下标为0: l\n",
      "第66次sample的next_token下标为0: l\n",
      "第67次sample的next_token下标为1: j\n",
      "第68次sample的next_token下标为0: l\n",
      "第69次sample的next_token下标为1: j\n",
      "第70次sample的next_token下标为0: l\n",
      "第71次sample的next_token下标为0: l\n",
      "第72次sample的next_token下标为1: j\n",
      "第73次sample的next_token下标为1: j\n",
      "第74次sample的next_token下标为1: j\n",
      "第75次sample的next_token下标为0: l\n",
      "第76次sample的next_token下标为1: j\n",
      "第77次sample的next_token下标为1: j\n",
      "第78次sample的next_token下标为0: l\n",
      "第79次sample的next_token下标为0: l\n",
      "第80次sample的next_token下标为0: l\n",
      "第81次sample的next_token下标为0: l\n",
      "第82次sample的next_token下标为1: j\n",
      "第83次sample的next_token下标为1: j\n",
      "第84次sample的next_token下标为1: j\n",
      "第85次sample的next_token下标为1: j\n",
      "第86次sample的next_token下标为0: l\n",
      "第87次sample的next_token下标为0: l\n",
      "第88次sample的next_token下标为0: l\n",
      "第89次sample的next_token下标为1: j\n",
      "第90次sample的next_token下标为0: l\n",
      "第91次sample的next_token下标为0: l\n",
      "第92次sample的next_token下标为1: j\n",
      "第93次sample的next_token下标为1: j\n",
      "第94次sample的next_token下标为1: j\n",
      "第95次sample的next_token下标为0: l\n",
      "第96次sample的next_token下标为0: l\n",
      "第97次sample的next_token下标为0: l\n",
      "第98次sample的next_token下标为0: l\n",
      "第99次sample的next_token下标为0: l\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "dict_map = {0:\"l\", 1:\"j\", 2:\"h\"}\n",
    "prob = torch.tensor([0.3, 0.3, 0])\n",
    "print(\"prob : \",prob)\n",
    "\n",
    "print(\"根据概率权重所选择next token的下标\")\n",
    "for i in range(100):\n",
    "    next_token = torch.multinomial(prob, num_samples=1)[0]\n",
    "    print(f\"第{i}次sample的next_token下标为{next_token}:\", \n",
    "          dict_map[int(next_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
