{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import LlaMA,LlaMAConfig\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby_llama config LlaMAConfig(block_size=1024, vocab_size=32000, padded_vocab_size=32000, n_layer=2, n_head=8, n_embd=128)\n"
     ]
    }
   ],
   "source": [
    "# 加载模型配置\n",
    "config = LlaMAConfig.from_name(\"baby_llama\")\n",
    "config.block_size = block_size\n",
    "config.vocab_size = 32000\n",
    "config.padded_vocab_size = 32000\n",
    "print(\"baby_llama config\",config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlaMA(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(32000, 128)\n",
      "    (h): ModuleList(\n",
      "      (0-1): 2 x Block(\n",
      "        (rms_1): RMSNorm()\n",
      "        (attn): CasualSelfAttention(\n",
      "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (rms_2): RMSNorm()\n",
      "        (mlp): MLP(\n",
      "          (fc_1): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (fc_2): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 根据模型配置加载模型\n",
    "model = LlaMA(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape: torch.Size([16, 1024])\n",
      "target.shape: torch.Size([16, 1024])\n",
      "tensor([41, 18,  3, 39,  3, 22, 38, 11, 63, 14])\n",
      "tensor([18,  3, 39,  3, 22, 38, 11, 63, 14, 33])\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "input = torch.load(\"./input.pt\")\n",
    "target = torch.load(\"./target.pt\")\n",
    "print(\"input.shape:\",input.shape)\n",
    "print(\"target.shape:\",target.shape)\n",
    "print(input[0,:10])\n",
    "print(target[0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape: torch.Size([16, 1024, 32000])\n",
      "vocab_size: 32000\n",
      "loss: tensor(10.5448, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 推理\n",
    "logits = model(input)\n",
    "print(\"logits.shape:\",logits.shape)\n",
    "print(\"vocab_size:\",config.vocab_size)\n",
    "loss = torch.nn.functional.cross_entropy(logits.view(-1,config.vocab_size),target.view(-1),ignore_index=-1)\n",
    "print(\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:16, length1024 \n",
      "---------------0. create RoPE, Mask----------------\n",
      "rope.shape: torch.Size([1024, 8, 2])\n",
      "mask.shape: torch.Size([1, 1, 1024, 1024])\n",
      "max_seq_length: 1024\n",
      "---------------1.embding----------------\n",
      "n_embd:  128\n",
      "before embeding:  torch.Size([16, 1024])\n",
      "after embeding:  torch.Size([16, 1024, 128])\n",
      "---------------2.llama block attention ----------------\n",
      "block_size: 2\n",
      "n_layers: 2\n",
      "Llama Block: torch.Size([16, 1024, 128])\n",
      "Llama Block: torch.Size([16, 1024, 128])\n",
      "---------------3.llama output ----------------\n",
      "rms_norm_out: torch.Size([16, 1024, 128])\n",
      "output logits: torch.Size([16, 1024, 32000])\n",
      "vocab_size: 32000\n"
     ]
    }
   ],
   "source": [
    "# model forward stepbystep\n",
    "idx = input\n",
    "B, T = idx.size()\n",
    "print(\"batch:{}, length{} \".format(B,T))\n",
    "print('---------------0. create RoPE, Mask----------------')\n",
    "# 创建RoPE、mask矩阵\n",
    "rope = model.rope_cache[:T,:T]\n",
    "mask = model.mask_cache[:,:,:T,:T]\n",
    "print(\"rope.shape:\",rope.shape)\n",
    "print(\"mask.shape:\",mask.shape)\n",
    "max_seq_length = config.block_size\n",
    "print(\"max_seq_length:\",max_seq_length)\n",
    "\n",
    "\n",
    "print('---------------1.embding----------------')\n",
    "x = model.transformer.wte(idx)\n",
    "x_embd = x\n",
    "print(\"n_embd: \", config.n_embd)\n",
    "print(\"before embeding: \", idx.shape)\n",
    "print(\"after embeding: \", x.shape)\n",
    "\n",
    "print('---------------2.llama block attention ----------------')\n",
    "print(\"block_size:\",len(model.transformer.h))\n",
    "print(\"n_layers:\",config.n_layer)\n",
    "for block in model.transformer.h:\n",
    "    x,_ = block(x,rope,mask,max_seq_length)\n",
    "    print(\"Llama Block:\",x.shape)\n",
    "\n",
    "print('---------------3.llama output ----------------')\n",
    "x = model.transformer.ln_f(x)\n",
    "print(\"rms_norm_out:\",x.shape)\n",
    "logits = model.lm_head(x)\n",
    "print(\"output logits:\",logits.shape)\n",
    "print(\"vocab_size:\",config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(\n",
      "  (rms_1): RMSNorm()\n",
      "  (attn): CasualSelfAttention(\n",
      "    (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "    (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (rms_2): RMSNorm()\n",
      "  (mlp): MLP(\n",
      "    (fc_1): Linear(in_features=128, out_features=512, bias=False)\n",
      "    (fc_2): Linear(in_features=128, out_features=512, bias=False)\n",
      "    (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "rms_1 -> attention-> rms_2-> MLP\n"
     ]
    }
   ],
   "source": [
    "# debug block 结构\n",
    "block = model.transformer.h[0]\n",
    "print(block)\n",
    "x,_ = block(x,rope,mask,max_seq_length)\n",
    "print(\"rms_1 -> attention-> rms_2-> MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block attention result: torch.Size([16, 1024, 128])\n",
      "x+mlp(x) result: torch.Size([16, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "# block forward\n",
    "block = model.transformer.h[0]\n",
    "x = x_embd\n",
    "x_rms_1 = block.rms_1(x)\n",
    "x_attn, _ = block.attn(x_rms_1, rope, mask, max_seq_length, None, None)\n",
    "x = x_embd + x_attn\n",
    "print('block attention result:', x.shape)\n",
    "\n",
    "x_rms_2 = block.rms_2(x)\n",
    "x_block_out = x + block.mlp(x_rms_2)\n",
    "print('x+mlp(x) result:', x_block_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSNorm()\n",
      "rms_norm.scale torch.Size([128])\n",
      "config.n_embd 128\n",
      "rms_norm.eps 1e-05\n",
      "rms_norm.dim -1\n",
      "归一化前 torch.Size([16, 1024, 128])\n",
      "归一化后 torch.Size([16, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "# rms_norm 实现\n",
    "rms_norm = model.transformer.h[0].rms_1\n",
    "print(rms_norm)\n",
    "x = x_embd\n",
    "print(\"rms_norm.scale\", rms_norm.scale.shape)\n",
    "print(\"config.n_embd\", config.n_embd)\n",
    "print(\"rms_norm.eps\", rms_norm.eps)\n",
    "print(\"rms_norm.dim\", rms_norm.dim)\n",
    "norm_x = torch.mean(x * x, dim=rms_norm.dim, keepdim=True)\n",
    "x_normed = x * torch.rsqrt(norm_x + rms_norm.eps)\n",
    "x_rms = rms_norm.scale * x_normed\n",
    "print(\"归一化前\", x_embd.shape)\n",
    "print(\"归一化后\", x_rms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入:句长:1024,元素个数:128\n",
      "idx_theta.shape: torch.Size([1024, 64])\n",
      "cache.shape: torch.Size([1024, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "# simple rope\n",
    "seq_len = block_size\n",
    "n_elem = config.n_embd\n",
    "base = 10000\n",
    "print(f\"输入:句长:{seq_len},元素个数:{n_elem}\")\n",
    "theta = 1.0 / (10000 ** (torch.arange(0,n_elem,2)) / n_elem)\n",
    "pos = torch.arange(seq_len)\n",
    "idx_theta = torch.outer(pos,theta)\n",
    "print(\"idx_theta.shape:\",idx_theta.shape)\n",
    "cache = torch.stack([torch.cos(idx_theta),torch.sin(idx_theta)],dim=-1)\n",
    "print(\"cache.shape:\",cache.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入:句长:1024,单头维度:16\n",
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14])\n",
      "theta: tensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n",
      "        1.0000e-03, 3.1623e-04])\n",
      "seqidx: tensor([   0,    1,    2,  ..., 1021, 1022, 1023])\n",
      "position idx* theta : torch.Size([1024, 8])\n",
      "idx_theta[:4,:4]: tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.3162, 0.1000, 0.0316],\n",
      "        [2.0000, 0.6325, 0.2000, 0.0632],\n",
      "        [3.0000, 0.9487, 0.3000, 0.0949]])\n",
      "cache:  torch.Size([1024, 8, 2])\n",
      "tensor([[0.5403, 0.8415],\n",
      "        [0.9504, 0.3110],\n",
      "        [0.9950, 0.0998],\n",
      "        [0.9995, 0.0316]])\n",
      "torch.Size([1024, 8, 2])\n",
      "<built-in method type of Tensor object at 0x7f14697d5210>\n"
     ]
    }
   ],
   "source": [
    "# 创建RoPE位置编码\n",
    "RoPECache = torch.Tensor\n",
    "\n",
    "# print(\"输入:句长,单头维度\")\n",
    "def build_rope_cache(\n",
    "    seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n",
    ") -> RoPECache:\n",
    "    \n",
    "    print(\"输入:句长:{},单头维度:{}\".format(seq_len, n_elem))\n",
    "    \n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
    "    print(torch.arange(0, n_elem, 2, dtype=dtype, device=device))\n",
    "    print(\"theta:\", theta)\n",
    "\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "    print(\"seqidx:\", seq_idx)\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "    print(\"position idx* theta :\", idx_theta.shape)\n",
    "    print(\"idx_theta[:4,:4]:\", idx_theta[:4,:4])\n",
    "\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "    print(\"cache: \", cache.shape)\n",
    "    print(cache[1,:4,:2])\n",
    "\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half()\n",
    "    print(cache.shape)\n",
    "    print(cache.type)\n",
    "    return cache\n",
    "\n",
    "# Rope 实现\n",
    "RoPECache = build_rope_cache(\n",
    "    seq_len=model.config.block_size,\n",
    "    n_elem=model.config.n_embd // model.config.n_head,\n",
    "    dtype=idx.dtype,\n",
    "    device=idx.device,\n",
    ")\n",
    "\n",
    "model.RoPECache = RoPECache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x: torch.Tensor, rope_cache: RoPECache) -> torch.Tensor:\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T] # [T,head_size // 2,2]\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2) # [bs,seq,head_num,head_size // 2,2]\n",
    "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2) # [1,seq,1,head_size // 2,2]\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return x_out2.type_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CasualSelfAttention(\n",
      "  (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "torch.Size([16, 1024, 128])\n",
      "batch:16, length:1024, n_embding:128\n",
      "--------------1. attenion split------------------\n",
      "batch, length, head: n_embding: torch.Size([16, 1024, 8, 16])\n",
      "--------------2. qk RoPE 旋转相对位置编码------------------\n",
      "RoPE编码作用在每个block的attention计算QK里\n",
      "q_rope前: torch.Size([16, 1024, 8, 16])\n",
      "q_rope后: torch.Size([16, 1024, 8, 16])\n",
      "--------------3. 计算scale dot product 和前向传播------------------\n",
      "attention output: torch.Size([16, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "# block attention 实现\n",
    "\n",
    "block_attn = model.transformer.h[0].attn\n",
    "print(block_attn)\n",
    "\n",
    "x_attn, _ = block_attn(x_rms_1, rope, mask, max_seq_length, None, None)\n",
    "print(x_attn.shape)\n",
    "\n",
    "x = x_rms_1\n",
    "B, T, C = x.size()\n",
    "print(\"batch:{}, length:{}, n_embding:{}\".format(B,T,C))\n",
    "\n",
    "print('--------------1. attenion split------------------')\n",
    "q, k, v = block_attn.c_attn(x).split(block_attn.n_embd, dim=2)\n",
    "head_size = C // block_attn.n_head\n",
    "k = k.view(B, T, block_attn.n_head, head_size)\n",
    "q = q.view(B, T, block_attn.n_head, head_size)\n",
    "v = v.view(B, T, block_attn.n_head, head_size)\n",
    "print(\"batch, length, head: n_embding: {}\".format(k.shape))\n",
    "\n",
    "print('--------------2. qk RoPE 旋转相对位置编码------------------')\n",
    "print('RoPE编码作用在每个block的attention计算QK里')\n",
    "q_rope_before = q\n",
    "q = apply_rope(q, rope)\n",
    "q_rope_after = q\n",
    "k = apply_rope(k, rope)\n",
    "print(\"q_rope前:\", q_rope_before.shape)\n",
    "print(\"q_rope后:\", q_rope_after.shape)\n",
    "\n",
    "k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "print('--------------3. 计算scale dot product 和前向传播------------------')\n",
    "y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n",
    "y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "# output projection\n",
    "y = block_attn.c_proj(y)\n",
    "print(\"attention output:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc_1): Linear(in_features=128, out_features=512, bias=False)\n",
      "  (fc_2): Linear(in_features=128, out_features=512, bias=False)\n",
      "  (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "SiLU(x) = x * sigmoid(x)\n",
      "c_fc1 is gate\n",
      "c_fc2 is up\n",
      "mlp output: torch.Size([16, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "## mlp silu\n",
    "mlp = model.transformer.h[0].mlp\n",
    "print(mlp)\n",
    "x = x_rms_1\n",
    "print(\"SiLU(x) = x * sigmoid(x)\")\n",
    "x = F.silu(mlp.fc_1(x)) * mlp.fc_2(x) \n",
    "print(\"c_fc1 is gate\")\n",
    "print(\"c_fc2 is up\")\n",
    "x = mlp.c_proj(x)\n",
    "print(\"mlp output:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
